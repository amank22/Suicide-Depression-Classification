{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23024109",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "!apt-get -y update && apt-get -y install \\\n",
    "    git \\\n",
    "    libsndfile1 \\\n",
    "    cmake \\\n",
    "    libcudnn7=7.6.5.32-1+cuda10.1 \\\n",
    "    libnccl2=2.7.8-1+cuda10.1 \\\n",
    "    libnccl-dev=2.7.8-1+cuda10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c4212",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdfca5",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "test_url = \"https://sagemaker-us-east-1-249959045939.s3.amazonaws.com/suicide-transformer/suicide-classification/suicide-classify-trial/data/test/test.csv\"\n",
    "train_url = \"https://sagemaker-us-east-1-249959045939.s3.amazonaws.com/suicide-transformer/suicide-classification/suicide-classify-trial/data/training/train.csv\"\n",
    "validate_url = \"https://sagemaker-us-east-1-249959045939.s3.amazonaws.com/suicide-transformer/suicide-classification/suicide-classify-trial/data/validation/validation.csv\"\n",
    "!wget $test_url $train_url $validate_url  -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77001e9d",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "!git clone --depth=1 https://github.com/ludwig-ai/ludwig.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df137b",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y ludwig\n",
    "!pip cache remove ludwig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a072c",
   "metadata": {
    "gradient": {
     "editing": true
    }
   },
   "outputs": [],
   "source": [
    "!cd ludwig/ \\\n",
    "    && HOROVOD_GPU_OPERATIONS=NCCL \\\n",
    "       HOROVOD_WITH_TENSORFLOW=1 \\\n",
    "       HOROVOD_WITHOUT_MPI=1 \\\n",
    "       HOROVOD_WITHOUT_PYTORCH=1 \\\n",
    "       HOROVOD_WITHOUT_MXNET=1 \\\n",
    "    && pip install --no-cache-dir '.[text,audio,image,hyperopt,serve,viz]'\n",
    "# !cd ludwig && pip install --no-cache-dir '.[full]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78992d",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!pip uninstall -y horovod # uninstall horovod to get ludwig to work correctly in notebook\n",
    "!pip install -U ludwig[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab4e56",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy pandas petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ludwig/requirements_dask.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d71616",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "!pip list | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72376d73",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def read_csv(input_dir):\n",
    "    # Take the set of 1 or more files and read them all into a single pandas dataframe\n",
    "    input_files = [ os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('csv') ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(f'No csv files found in {input_dir}')\n",
    "    df = pd.concat([pd.read_csv(file) for file in input_files])\n",
    "    print(f'Loaded {len(input_files)} files from {input_dir}, shape: {df.shape}')\n",
    "    return df\n",
    "    \n",
    "def train(args):\n",
    "    # output directories\n",
    "    print(f'input train: {args.training_set}, val: {args.validation_set}, test: {args.testing_set}')\n",
    "    print(f'output model: {args.model_dir}, data: {args.output_data_dir}')\n",
    "\n",
    "    # configure integrations https://ludwig-ai.github.io/ludwig-docs/user_guide/#integrations\n",
    "    try:\n",
    "        import ludwig.contrib\n",
    "        if args.integration == 'comet':\n",
    "            ludwig.contrib.use_contrib('comet')\n",
    "            print(f'using comet integration')\n",
    "        elif args.integration == 'wandb':\n",
    "            ludwig.contrib.use_contrib('wandb')\n",
    "            print(f'using wandb integration')\n",
    "    except Exception as e:\n",
    "        print('integration not supported: {}'.format(e))\n",
    "\n",
    "    # import ludwig after contrib incase we need to hook TF prior to loading\n",
    "    from ludwig.api import LudwigModel\n",
    "    \n",
    "    # loading csv dataframes\n",
    "    train_df = pd.read_csv(args.training_set)\n",
    "    val_df = pd.read_csv(args.validation_set)\n",
    "    test_df = pd.read_csv(args.testing_set)\n",
    "    \n",
    "    # train the model based on config yaml file\n",
    "    ludwig_model = LudwigModel(args.config, logging_level=logging.DEBUG, gpus=0)\n",
    "    train_stats, _, _  = ludwig_model.train(\n",
    "        experiment_name=args.experiment_name,\n",
    "        model_name=args.model_name,\n",
    "        training_set=train_df,\n",
    "        validation_set=val_df,\n",
    "        test_set=test_df,\n",
    "        output_directory = args.output_data_dir, # Save experiment to output data dir\n",
    "        skip_save_training_statistics=False, # Save training results to file\n",
    "        skip_save_log=False, # Save tensorboard logs\n",
    "        skip_save_progress = False,\n",
    "    )\n",
    "    \n",
    "    print('saving model')\n",
    "    \n",
    "    # Save the latest model to model_directory\n",
    "    ludwig_model.save(args.model_dir)\n",
    "    \n",
    "    # Save the compiled SavedModel to model directory\n",
    "    ludwig_model.save_savedmodel(args.model_dir)\n",
    "    \n",
    "    print('emmiting metrics')\n",
    "    \n",
    "    # enuemrate through the channels and output features to get metrics\n",
    "    for channel in train_stats:\n",
    "        for output in ludwig_model.config['output_features']:\n",
    "            for metric in train_stats['training'][output['name']]:\n",
    "                # get the metric from last epoch\n",
    "                value = train_stats['training'][output['name']][metric][-1]\n",
    "                print('{}_{}={};'.format(channel, metric, value))\n",
    "    \n",
    "\n",
    "    print('evaluating test dataset')\n",
    "        \n",
    "    # output evaluations based on test\n",
    "    ludwig_model.evaluate(test_df,\n",
    "        output_directory=args.output_data_dir,\n",
    "        skip_save_unprocessed_output=True, # Only save CSV values\n",
    "        skip_save_predictions=False, # Write predictions to file\n",
    "        skip_save_eval_stats=False, # Write evaluation stats to file\n",
    "        collect_predictions=True,\n",
    "        collect_overall_stats=True,\n",
    "    )\n",
    "    \n",
    "    # Return the model \n",
    "    return ludwig_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # reads input channels training and testing from the environment variables\n",
    "    parser.add_argument(\"--config\", type=str, default='config.yml')\n",
    "    parser.add_argument(\"--training-set\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--validation-set\", type=str, default=os.environ[\"SM_CHANNEL_VALIDATION\"])\n",
    "    parser.add_argument(\"--testing-set\", type=str, default=os.environ[\"SM_CHANNEL_TESTING\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--experiment-name\", type=str, default='api_experiment')\n",
    "    parser.add_argument(\"--model-name\", type=str, default='run')\n",
    "    parser.add_argument(\"--integration\", type=str, required=False)\n",
    "    args = parser.parse_args()\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168b1fb",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "%%writefile config.yml\n",
    "input_features:\n",
    "    -   name: text\n",
    "        type: text\n",
    "        level: word\n",
    "        encoder: distilbert\n",
    "output_features:\n",
    "    -   name: class\n",
    "        type: category\n",
    "training:\n",
    "    epochs: 2\n",
    "    batch_size: 16 # OOM for bert if we don't keep this small\n",
    "    learning_rate: 0.00001\n",
    "    decay: true\n",
    "    trainable: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6c94d",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "%env SM_CHANNEL_TRAINING=data/train.csv\n",
    "%env SM_CHANNEL_VALIDATION=data/validation.csv\n",
    "%env SM_CHANNEL_TESTING=data/test.csv\n",
    "%env SM_MODEL_DIR=model/\n",
    "%env SM_OUTPUT_DATA_DIR=output/\n",
    "!python train.py --config config.yml --experiment-name=suicide-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3b8be",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}